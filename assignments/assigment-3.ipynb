{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f457df4-a840-4838-9b30-fc5feb2c09b5",
   "metadata": {},
   "source": [
    "# Assignment 3: Neural networks in natural language processing\n",
    "\n",
    "### Due Date: Oct 30 (both sections)\n",
    "\n",
    "### Grade (100 pts, 10%)\n",
    "\n",
    "#### Your Name:\n",
    "\n",
    "#### Your EID:\n",
    "\n",
    "*Note: This assignment covers material from the recording, notes, demo, and suggested readings from Lecture-08*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5040c21a-f813-4ce5-b5d7-bbdd59e37dcc",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369615c-54d7-4a72-bfe2-6fd84cef9582",
   "metadata": {},
   "source": [
    "### 1. Dropout (50 pts)\n",
    "\n",
    "Dropout is a regularization technique that randomly sets units in each activation layer, $a \\in \\mathbb{R}^{D}$, to zero and then multiplies the resultant vector elementwise by a constant $\\gamma$ according to:\n",
    "\n",
    "$$a_{dropout} \\leftarrow  \\gamma H \\odot a$$\n",
    "\n",
    "where $\\odot$ represents the element-wise product operator and $H \\in \\{0, 1\\}^D$ is a mask with entries drawn from \n",
    "\n",
    "$$\\begin{cases} p(0) &= p_{dropout} \\\\ p(1) &= 1 - p_{dropout} \\end{cases}$$\n",
    "\n",
    "Select a scaling factor ${\\gamma}$ that ensures the expected value over the activation layer remains invariant to the above operation, $E\\big[ a_{dropout} \\big] = E\\big[ a \\big]$, and provide rationale for your selection.\n",
    "\n",
    "*Hint: You want to show that*\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^D a_i = \\gamma \\sum_{i=1}^D a_{dropout, i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08cbe0-e13b-4325-a3b8-363c78a5fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2785529-1d06-4474-8aec-baef38545787",
   "metadata": {},
   "source": [
    "### 2. Convolutions (50 pts)\n",
    "\n",
    "Consider a sequence of $T$ token embeddings, $Z \\in \\mathbb{R}^{T \\times D}$, for which $D=3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347fcde5-c66a-4b4d-a7de-8491b5fcc513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Z = np.array([\n",
    "    [1.3,   0.4, -0.2],\n",
    "    [-3.1,  1.1,  2.1],\n",
    "    [0.9,   2.8, -1.5],\n",
    "    [1.3,   2.4,  0.1],\n",
    "    [1.0,   1.0,  0.5],\n",
    "    [3.0,  -1.4, -0.2],\n",
    "    [-0.7,  1.8,  1.3]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3cbe3-5b3a-44aa-886f-9f56e126e04d",
   "metadata": {},
   "source": [
    "and a set of convolutional filters, $W=\\{ w^{(1)}, w^{(2)} \\}$, and corresponding filter widths $S=\\{ s^{(1)}, s^{(2)}  \\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d0a4d-57ea-42c9-97d1-60055b3fbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "w2 = np.array([\n",
    "    [2, 2, 2],\n",
    "    [2, 2, 2],\n",
    "    [2, 2, 2]\n",
    "])\n",
    "\n",
    "W = [w1, w2]\n",
    "\n",
    "S = [2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c6909-6c25-4dde-8a87-fa79b27dbf3e",
   "metadata": {},
   "source": [
    "In Lecture 08 we discussed a set of operations that maps $Z \\in \\mathbb{R}^{T \\times D}$ onto $Z' \\in \\mathbb{R}^{N_F D}$ (in this problem $N_F = 2$). This involved three steps:\n",
    "\n",
    "1. **Convolution**: The convolutional operation produces $N_F$ feature maps, $B^{(n)} \\in \\mathbb{R}^{(T - s^{(n)} + 1) \\times D}$, where $n=\\{1, \\dots, N_F\\}$, according to:\n",
    "\n",
    "$$\n",
    "\\forall_{t \\in \\{ 1, \\dots, T - s^{(n)} + 1 \\} } \\; B^{(n)}_{t,j} = \\sum_{t'=1}^{S^{(n)}} w^{(n)}_{t',j} \\; Z_{t,j}\n",
    "$$\n",
    "\n",
    "2. **Max pooling**: The max pooling operation computes the max over the sequence dimension in each feature map, $ B_{maxpool}^{(n)} \\in \\mathbb{R}^D$, according to:\n",
    "\n",
    "$$\n",
    "B_{maxpool, j}^{(n)} = \\underset{1 \\leq t' \\leq T - s^{(n)} + 1 }{\\max} B^{(n)}_{t', j}\n",
    "$$\n",
    "\n",
    "3. **Concatenation**: The resultant set of $N_F$ feature vectors are then concatenated into a single vector $Z'$ according to:\n",
    "\n",
    "$$\n",
    "Z' = \\big[ B_{maxpool}^{(1)}, \\dots, B_{maxpool}^{(n)}, \\dots,  B_{maxpool}^{(N_F)}  \\big] \\in \\mathbb{R}^{D \\cdot N_F}\n",
    "$$\n",
    "\n",
    "In the cell below, perform these three operations to produce $Z' \\in \\mathbb{R}^6$ and print it.\n",
    "\n",
    "*Hint: The max pooling operation computes the maximum over each column in $B^{(n)}$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7747a3-da78-44e9-ae19-c3f464bd9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
