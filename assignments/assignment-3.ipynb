{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Neural networks in natural language processing\n",
    "\n",
    "### Due Date: Oct 30 (both sections)\n",
    "\n",
    "### Grade (100 pts, 10%)\n",
    "\n",
    "#### Your Name: Gabriella Zakrocki\n",
    "\n",
    "#### Your EID: gz120\n",
    "\n",
    "*Note: This assignment covers material from the recording, notes, demo, and suggested readings from Lecture-08*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dropout (50 pts)\n",
    "\n",
    "Dropout is a regularization technique that randomly sets units in each activation layer, $a \\in \\mathbb{R}^{D}$, to zero and then multiplies the resultant vector elementwise by a constant $\\gamma$ according to:\n",
    "\n",
    "$$a_{dropout} \\leftarrow  \\gamma H \\odot a$$\n",
    "\n",
    "where $\\odot$ represents the element-wise product operator and $H \\in \\{0, 1\\}^D$ is a mask with entries drawn from \n",
    "\n",
    "$$\\begin{cases} p(0) &= p_{dropout} \\\\ p(1) &= 1 - p_{dropout} \\end{cases}$$\n",
    "\n",
    "Select a scaling factor ${\\gamma}$ that ensures the expected value over the activation layer remains invariant to the above operation, $E\\big[ a_{dropout} \\big] = E\\big[ a \\big]$, and provide rationale for your selection.\n",
    "\n",
    "*Hint: You want to show that*\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^D a_i = \\gamma \\sum_{i=1}^D a_{dropout, i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling factor would need to be $ {\\gamma} = 1/(1-p) $ \n",
    "$$ E\\big[ a_{dropout} \\big] = H [* p(0) * a_i * 0 + p(1) * a_i *1 = p * a_i * 0 + (1-p) * a_i*1 $$\n",
    "$$ E\\big[ a_{dropout} \\big] = (1-p)*a_i $$\n",
    "If we want $ E\\big[ a_{dropout} \\big] =  E\\big[ a \\big]$ then we see that if  $E\\big[ a \\big] = a_i$ and \n",
    "$E\\big[ a_{dropout} \\big] = (1-p)*a_i$ so the solution that exists that ensures the expected value over the activation layer remains invariant is $ {\\gamma} = 1/(1-p) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convolutions (50 pts)\n",
    "\n",
    "Consider a sequence of $T$ token embeddings, $Z \\in \\mathbb{R}^{T \\times D}$, for which $D=3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Z = np.array([\n",
    "    [1.3,   0.4, -0.2],\n",
    "    [-3.1,  1.1,  2.1],\n",
    "    [0.9,   2.8, -1.5],\n",
    "    [1.3,   2.4,  0.1],\n",
    "    [1.0,   1.0,  0.5],\n",
    "    [3.0,  -1.4, -0.2],\n",
    "    [-0.7,  1.8,  1.3]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a set of convolutional filters, $W=\\{ w^{(1)}, w^{(2)} \\}$, and corresponding filter widths $S=\\{ s^{(1)}, s^{(2)}  \\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "w2 = np.array([\n",
    "    [2, 2, 2],\n",
    "    [2, 2, 2],\n",
    "    [2, 2, 2]\n",
    "])\n",
    "\n",
    "W = [w1, w2]\n",
    "\n",
    "S = [2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lecture 08 we discussed a set of operations that maps $Z \\in \\mathbb{R}^{T \\times D}$ onto $Z' \\in \\mathbb{R}^{N_F D}$ (in this problem $N_F = 2$). This involved three steps:\n",
    "\n",
    "1. **Convolution**: The convolutional operation produces $N_F$ feature maps, $B^{(n)} \\in \\mathbb{R}^{(T - s^{(n)} + 1) \\times D}$, where $n=\\{1, \\dots, N_F\\}$, according to:\n",
    "\n",
    "$$\n",
    "\\forall_{t \\in \\{ 1, \\dots, T - s^{(n)} + 1 \\} } \\; B^{(n)}_{t,j} = \\sum_{t'=1}^{S^{(n)}} w^{(n)}_{t',j} \\; Z_{t+t'-1,j}\n",
    "$$\n",
    "\n",
    "2. **Max pooling**: The max pooling operation computes the max over the sequence dimension in each feature map, $ B_{maxpool}^{(n)} \\in \\mathbb{R}^D$, according to:\n",
    "\n",
    "$$\n",
    "B_{maxpool, j}^{(n)} = \\underset{1 \\leq t' \\leq T - s^{(n)} + 1 }{\\max} B^{(n)}_{t', j}\n",
    "$$\n",
    "\n",
    "3. **Concatenation**: The resultant set of $N_F$ feature vectors are then concatenated into a single vector $Z'$ according to:\n",
    "\n",
    "$$\n",
    "Z' = \\big[ B_{maxpool}^{(1)}, \\dots, B_{maxpool}^{(n)}, \\dots,  B_{maxpool}^{(N_F)}  \\big] \\in \\mathbb{R}^{D \\cdot N_F}\n",
    "$$\n",
    "\n",
    "In the cell below, perform these three operations to produce $Z' \\in \\mathbb{R}^6$ and print it.\n",
    "\n",
    "*Hint: The max pooling operation computes the maximum over each column in $B^{(n)}$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '[' (<ipython-input-21-f57abefdc6cb>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-f57abefdc6cb>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    B_conv1.append(Z[0]*(W[S[0])+Z[1]*(W[S[0]])+Z[2]*(W[S[0]])+Z[3]*(W[S[0]])+Z[4]*(W[S[0]])+Z[5]*(W[S[0]])\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '['\n"
     ]
    }
   ],
   "source": [
    "# Convolution\n",
    "\n",
    "B_conv1 = []\n",
    "B_conv2 = []\n",
    "\n",
    "for w, s in zip(W,S):\n",
    "    B_conv1.append((Z[0]*(W[S[0]]))+(Z[1]*(W[S[0]]))+(Z[2]*(W[S[0]]))+(Z[3]*(W[S[0]]))+(Z[4]*(W[S[0]]))+(Z[5]*(W[S[0]])))\n",
    "\n",
    "for w, s in zip(W,S):\n",
    "    B_conv2.append((Z[0]*(W[S[1]]))+(Z[1]*(W[S[1]]))+(Z[2]*(W[S[1]]))+(Z[3]*(W[S[1]]))+(Z[4]*(W[S[1]]))+(Z[5]*(W[S[1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max pooling\n",
    "\n",
    "B_maxpool_1 = max(0, B_conv1)\n",
    "B_maxpool_2 = max(0, B_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_prime = [B_maxpool_1, B_maxpool_2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
