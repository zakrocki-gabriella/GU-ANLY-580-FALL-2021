{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bfd3b9f-a737-49cb-8bcd-2d10d73e7330",
   "metadata": {},
   "source": [
    "# Assignment 4 (solutions): Attention\n",
    "\n",
    "### Due Date: Nov 22 (both sections)\n",
    "\n",
    "### Grade (100 pts, 10%)\n",
    "\n",
    "#### Your Name:\n",
    "\n",
    "#### Your EID:\n",
    "\n",
    "*Note: This assignment covers material from the recording, notes, demo, and suggested readings from Lectures 5,8,9*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109a8ab-5790-442a-a2aa-f0c950acfb87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Problem introduction\n",
    "\n",
    "In this assignment we will explore the concept of attention using Word2Vec features. Recall that in lecture-05 we used the Word2Vec algorithm to compute feature representations of english words. For every center-word - context-word pair, ${\\textbf{x}_w, \\textbf{x}_c}$, in the training set, Word2Vec tries to maximize the inner product between their feature embeddings, $\\textbf{u}_w$ and $\\textbf{v}_c$, by first computing a vector of logits as $\\textbf{z} = \\textbf{u}_w \\cdot \\textbf{V}^T \\in \\mathbb{R}^N$, and then maximizing $p(\\textbf{x}_c | \\textbf{x}_w) = \\sigma_{softmax}(\\textbf{z})$ to learn $\\textbf{U}$ and $\\textbf{V}$ via MLE. \n",
    "\n",
    "Let's assume we are given a sequence of words:\n",
    "\n",
    "$$X = \\{ \\textbf{x}^{(1)}, \\textbf{x}^{(2)}, \\textbf{x}^{(3)}, \\textbf{x}^{(4)}, \\dots, \\textbf{x}^{(T)} \\}$$\n",
    "\n",
    "a corresponding sequence of center-word feature representations:\n",
    "\n",
    "$$U = \\{ \\textbf{u}^{(1)}, \\textbf{u}^{(2)}, \\textbf{u}^{(3)}, \\textbf{u}^{(4)}, \\dots, \\textbf{u}^{(T)} \\}$$\n",
    "\n",
    "and a corresponding context-word feature representations:\n",
    "\n",
    "$$V = \\{ \\textbf{v}^{(1)}, \\textbf{v}^{(2)}, \\textbf{v}^{(3)}, \\textbf{v}^{(4)}, \\dots, \\textbf{v}^{(T)} \\}$$\n",
    "\n",
    "Then, using the Word2Vec variable naming convention above, recall that in simple self attention (Lecture-09) we compute a set of *context vectors*:\n",
    "\n",
    "$$C = \\{ \\textbf{c}^{(1)}, \\textbf{c}^{(2)}, \\textbf{c}^{(3)}, \\textbf{c}^{(4)}, \\dots, \\textbf{c}^{(T)} \\}$$ \n",
    "\n",
    "each of which is computed according to:\n",
    "\n",
    "$$\n",
    "\\textbf{c}^{(t)} = \\sum_{t'=1}^{T} \\alpha_{t,t'} \\; \\textbf{u}^{(t)} \\quad where \\quad \\alpha_{t,t'} = \\frac{ e^{ \\textbf{u}^{(t)} \\cdot \\textbf{v}^{(t')} } }{ \\sum_{t''} e^{ \\textbf{u}^{(t)} \\cdot \\textbf{v}^{(t'')} } } \\\\\n",
    "$$\n",
    "\n",
    "The attention weights make up the following (non-symmetric!) matrix:\n",
    "\n",
    "$$\n",
    "\\textbf{A} = \n",
    "\\left[ \\begin{array}{ccc}\n",
    "\\alpha_{1,1} & \\dots & \\alpha_{1,T} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\alpha_{T,1} & \\dots & \\alpha_{T,T}\n",
    "\\end{array}\\right] \\; \\in [0,1]^{T \\times T} \\quad where \\quad \\sum_{t'} \\alpha_{t,t'} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62854825-7105-410a-9a73-87289cc349c6",
   "metadata": {},
   "source": [
    "### Test sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2649d1fc-1e1d-4f13-a1b3-500859e1b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Thomas Jefferson was an American statesman and Founding Father who served as the third president of the United States\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a6b869-1728-4d17-9b9c-816a07d8b6e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Word2Vec vocab, embedding matrices\n",
    "\n",
    "In the `assignments/a4/` folder you will find three .tsv files: a vocabulary (`metadata.tsv`), center-word Word2Vec embeddings (`vectors_center.tsv`), and context-word Word2Vec embeddings (`vectors_context.tsv`). These are taking from the Lecture-05 Word2Vec demo. The code below loads the center and context word embeddings for the above test sentence into two numpy arrays: $U$, and $V$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9da824f-7165-4aee-a88a-fbd9299de373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2321\n",
      "embed dim: 15\n",
      "center embedding lookup matrix shape: (15, 2321)\n",
      "context embedding lookup matrix shape: (15, 2321)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_matrix(fpath):\n",
    "    matrix = []\n",
    "    with open(fpath, 'r') as fd:\n",
    "        tsv = fd.read()\n",
    "    for line in tsv.split('\\n'):\n",
    "        row = []\n",
    "        for value in line.split('\\t'):\n",
    "            row.append(float(value))\n",
    "        matrix.append(row)\n",
    "    return np.array(matrix)\n",
    "\n",
    "def load_vocab(fpath) -> dict:\n",
    "    with open(fpath, \"r\") as fd:\n",
    "        tsv = fd.read()\n",
    "    vocab = {}\n",
    "    for line in tsv.split('\\n'):\n",
    "        vocab[line.strip()] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = load_vocab(\"metadata.tsv\")\n",
    "U_lookup = load_matrix(\"vectors_center.tsv\").T\n",
    "V_lookup = load_matrix(\"vectors_context.tsv\").T\n",
    "\n",
    "N = len(vocab)\n",
    "D = len(U_lookup)\n",
    "\n",
    "print(f\"vocab size: {N}\")\n",
    "print(f\"embed dim: {D}\")\n",
    "\n",
    "print(f\"center embedding lookup matrix shape: {U_lookup.shape}\")\n",
    "print(f\"context embedding lookup matrix shape: {V_lookup.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98c125dd-62da-4cf6-ab8c-5305e4d9260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indices: [2092, 1075, 2321, 2321, 79, 2321, 2321, 795, 2321, 2321, 2321, 2321, 2321, 2321, 1622, 2321, 2321, 2158, 1987]\n"
     ]
    }
   ],
   "source": [
    "word_list = sentence.lower().split(\" \")\n",
    "X = [vocab.get(word, N) for word in word_list]\n",
    "print(f\"Word indices: {X}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba6669c-910a-414c-adf4-f5a29889574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center word embedding lookup matrix w/oov embed added shape: (15, 2322)\n",
      "context word embedding lookup matrix w/oov embed added shape: (15, 2322)\n"
     ]
    }
   ],
   "source": [
    "# Assign OOV words low attention weights\n",
    "u_oov = 0.1 * np.mean(U_lookup, axis=1, keepdims=True)\n",
    "v_oov = 0.1 * np.mean(V_lookup, axis=1, keepdims=True) \n",
    "# Add the OOV embeddings to the lookup matrices\n",
    "U_lookup = np.concatenate((U_lookup, u_oov), axis=1)\n",
    "V_lookup = np.concatenate((V_lookup, v_oov), axis=1)\n",
    "U_lookup.shape, V_lookup.shape\n",
    "\n",
    "print(f\"center word embedding lookup matrix w/oov embed added shape: {U_lookup.shape}\")\n",
    "print(f\"context word embedding lookup matrix w/oov embed added shape: {U_lookup.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6dd2270-b227-4632-ad98-c25a53c02539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence of center-word embeddings shape: (15, 19)\n",
      "sequence of context-word embeddings shape: (15, 19)\n"
     ]
    }
   ],
   "source": [
    "# Center-word embedding sequence of X\n",
    "U = U_lookup[:, X]\n",
    "# Context-word embedding sequence of X\n",
    "V = V_lookup[:, X]\n",
    "\n",
    "print(f\"sequence of center-word embeddings shape: {U.shape}\")\n",
    "print(f\"sequence of context-word embeddings shape: {U.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdff31-c989-4ad4-9acc-9bc58d2d996f",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7175d4f-f87e-4c18-a97e-ef9ba463cfaf",
   "metadata": {},
   "source": [
    "### (40 pts) Q1: Attention weights\n",
    "\n",
    "1. Compute the attention weight matrix $\\textbf{A}$ for this test sentence using the definitions provided in the intro (and in Lecture-09 as needed).\n",
    "\n",
    "*Hint: You can use the batched `softmax()` function provided below to compute A.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a31692-e34a-494b-ac09-e130269678de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes a softmax over each row of Z\n",
    "    \"\"\"\n",
    "    Z_exp = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    partition = np.sum(Z_exp, axis=1, keepdims=True)\n",
    "    return Z_exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9caf63c9-3264-439c-b153-5037b289b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2266711c-3030-465d-a29a-48d11b010ee7",
   "metadata": {},
   "source": [
    "### (40 pts) Q2: Attention block\n",
    "\n",
    "1. Now compute the attention block $C$ using the attention weights that you've computed\n",
    "\n",
    "2. Compute the L2 norm of each of the resultant context vectors. Do the same for the center-word embeddings in the preceeding layer. How do the magnitudes compare? Is there a logical explanation for this?\n",
    "\n",
    "*Hint: You need to compute $\\textbf{c}^{(t)} \\; \\forall t \\in \\{1, \\dots, T\\}$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41733288-1899-429e-9567-2ed49d186eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4018f05-2d77-4059-8205-f05c30e1c9b3",
   "metadata": {},
   "source": [
    "### (20 pts) Q3: Visualize the attention weights\n",
    "\n",
    "Now that we've computed a simple self attention transformation of our Word2Vec sequence representation, it's time to visually examine what this attention mechanism affords us. First recognize that all values in $\\textbf{A}$ are in the range $[0,1]$, and that each row is normalized. Therefore, we really just need to generate a heatmap of this matrix, keeping in mind that the matrix indices correspond directly to the positions in the sequence.\n",
    "\n",
    "1. Generate the heatmap in the cell provided below\n",
    "\n",
    "2. Briefly describe/explain what you see in the heatmap.\n",
    "\n",
    "3. Why is our attention matrix not symmetric?\n",
    "\n",
    "*Hint: You can generate a heatmap of the attention weights by calling `plot_attention_weights(A)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e3a3572-90f1-49cc-a82f-e4234af68e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_attention_weights(att_matrix):\n",
    "    fig = px.imshow(A, x=word_list, y=word_list, width=20, height=20)\n",
    "    fig.update_layout(width=800, height=800)\n",
    "    print(sentence)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ce4bd7-1560-4a49-b443-65e6f7ef1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
