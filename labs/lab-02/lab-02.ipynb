{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e7a6f0",
   "metadata": {},
   "source": [
    "# Lab 02: Introduction to Text Preprocessing & the Spacy Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bf83d",
   "metadata": {},
   "source": [
    "### Objectives:\n",
    "1. Get familiar with basic text preprocessing pipelines\n",
    "2. Get familiar with regular expressions, and the `re` package in Python\n",
    "3. Evaluate the lexical diversity of the data in each category within the 20 News Groups Dataset\n",
    "4. Use normalized BOW features to evaluate text similarity using the KL-divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b408e5",
   "metadata": {},
   "source": [
    "### Required Reading:\n",
    "\n",
    "1. https://universaldependencies.org/u/pos/\n",
    "2. https://spacy.io/api/annotation#pos-tagging\n",
    "3. https://spacy.io/api/annotation#dependency-parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf47bf9",
   "metadata": {},
   "source": [
    "# Part I: Introduction to Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d9522",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download Spacy's base English language *pipeline* components\n",
    "\n",
    "``$ python -m spacy download en_core_web_sm``\n",
    "\n",
    "What is a Spacy *pipeline*? A Spacy pipeline is an extensible tool that streamlines many of the common tasks in NLP, such as tokenization, part-of-speech tagging, named entity recognition, stemming, lemmatizing, and parsing. It also has custom pipeline components specifically for transformers. It is built for production use; much thought and care has gone into its API and implementation. You can actually configure Spacy to use some of the statistical models that we will discover in this class; for now we're just going to cover some of the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b13e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "pipeline = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3006e3d",
   "metadata": {},
   "source": [
    "### Download the 20 News Groups dataset using the sklearn package\n",
    "\n",
    "This data consists of news articles from 20 different categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef0c922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "ng_train = fetch_20newsgroups(subset='train')\n",
    "ng_test = fetch_20newsgroups(subset='test')\n",
    "ng_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1dc63a",
   "metadata": {},
   "source": [
    "### Get the number of training & test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f48f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(ng_train.data), len(ng_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8c2a0c",
   "metadata": {},
   "source": [
    "### Take a peek at the first document and its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cdc06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = ng_train.target[0]\n",
    "ng_train.target_names[label_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29867bf9",
   "metadata": {},
   "source": [
    "### Evaluate Spacy's recognition of entities, POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3805db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "doc = pipeline(ng_train.data[0])\n",
    "for i, token in enumerate(doc):\n",
    "    pprint({\"text\": token.text,\n",
    "            \"lemma\": token.lemma_,\n",
    "            \"POS\": token.pos_,\n",
    "            \"tag\": token.tag_,\n",
    "            \"dep\": token.dep_,\n",
    "            \"shape\": token.shape_,\n",
    "            \"is_alpha\": token.is_alpha,\n",
    "            \"is_stop\": token.is_stop})\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c33e00-98d2-43b6-9b24-6bf3d919d6cd",
   "metadata": {},
   "source": [
    "### Visualize Spacy's dependency parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a2e99-ff4c-4aa1-89ce-5863b60b2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f788d",
   "metadata": {},
   "source": [
    "### Let's define a preprocessing function that cleans our data\n",
    "\n",
    "You'll notice that even the lemmatized text contains meaningless tokens. In the real world you're never going to get around having to do some feature engineering. In NLP this often means writing some regexes to transform text into a usable format. This has become less important in the deep learning era, but applying domain specific knowledge is always beneficial. In the case of this dataset, we have text that originated in news feeds, some of which is messy. There are email and url addresses, grammatical errors, and a lot puntuation and uninformative characters (e.g., the newline character `\\n`). Below is a function that does some very basic regex (regular expression) matching to strip out emails, urls, punctuation, and other junk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6991ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "# http://emailregex.com/\n",
    "email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "\n",
    "# replace = [ (pattern-to-replace, replacement),  ...]\n",
    "replace = [\n",
    "    (r\"<a[^>]*>(.*?)</a>\", r\"\\1\"),  # Matches most URLs\n",
    "    (email_re, \"email\"),            # Matches emails\n",
    "    (r\"(?<=\\d),(?=\\d)\", \"\"),        # Remove commas in numbers\n",
    "    (r\"\\d+\", \"numbr\"),              # Map digits to special token <numbr>\n",
    "    (r\"[\\t\\n\\r\\*\\.\\@\\,\\-\\/]\", \" \"),   # Punctuation and other junk\n",
    "    (r\"\\s+\", \" \")                   # Stips extra whitespace\n",
    "]\n",
    "\n",
    "train_text = ng_train.data\n",
    "test_text = ng_test.data\n",
    "for repl in replace:\n",
    "    train_text = [re.sub(repl[0], repl[1], text) for text in train_text]\n",
    "    test_text = [re.sub(repl[0], repl[1], text) for text in test_text]\n",
    "\n",
    "@Language.component(\"ng20\")\n",
    "def ng20_preprocess(doc):\n",
    "    tokens = [token for token in doc \n",
    "              if not any((token.is_stop, token.is_punct))]\n",
    "    tokens = [token.lemma_.lower().strip() for token in tokens]\n",
    "    tokens = [token for token in tokens if token]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "pipeline.add_pipe(\"ng20\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa40574",
   "metadata": {},
   "source": [
    "#### Peek at our processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2592485",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9878e",
   "metadata": {},
   "source": [
    "### Now pass each training and test document through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2975f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_train = [pipeline(doc) for doc in train_text[:500]]\n",
    "docs_test = [pipeline(doc) for doc in test_text[:500]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c51dd",
   "metadata": {},
   "source": [
    "### Let's look at that first document following this transformation and compare it to the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874cc543",
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2586e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf2c17",
   "metadata": {},
   "source": [
    "# Part II: Lexical diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931ec93",
   "metadata": {},
   "source": [
    "Sometimes it's useful to understand how diverse is the language in some body of text. Once simple heuristic to evaluate diversity is as follows: \n",
    "\n",
    "$$ lexical\\_diversity = \\frac{ len(set(all\\_words\\_in\\_doc)) }{ len(doc) }$$\n",
    "\n",
    "Find the set of all words observed in the document, and divide it by the number of total words in the document. Let's use this to evalute the diversity of each category in the 20NG dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cdd9cf",
   "metadata": {},
   "source": [
    "### (5 pts) Task I: \n",
    "In the cell below, compute the diversity of each category in the 20NG dataset using the above heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd07fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988f4f8",
   "metadata": {},
   "source": [
    "### Explain these scores: \n",
    "\n",
    "1. Is this result real or an artifact of some underlying problem with our data? \n",
    "2. What might you do to better evaluate lexical diversity on this data using this scoring function?\n",
    "3. Is this heuristic a good metric for lexical diversity in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86c35c",
   "metadata": {},
   "source": [
    "### Entropy \n",
    "Entropy is another, perhaps more principled, way by which we can evaluate how diverse, or varied, is a piece of text. Recall the definition of Entropy, $H(P(x))$:\n",
    "\n",
    "$$ H(P(x)) = \\sum_{i=1}^{N} -P(x_{i}) \\log P(x_{i}) $$\n",
    "\n",
    "In the Bag-of-Words (BOW) feature representation of a document, each document is represented by a word count vector, ${x}_{i} \\in \\mathbb{R}^{N}$ where $N$ is the cardinality of the set of words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c8563a-359b-4e5b-ab11-15162f6a30d4",
   "metadata": {},
   "source": [
    "### (5 pts) Task II:\n",
    "In order to compute an entropy from this representation, you'll first need to convert those count vectors into probability distributions. Then compute the entropy of the word distributions for each news category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b547a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ddbb6-57b3-4909-9c77-8cb837991873",
   "metadata": {},
   "source": [
    "### Explain this result\n",
    "\n",
    "1. What does it mean for a distribution to have high or low entropy?\n",
    "2. Do these scores make intuitive sense? Any more or less so than the heuristic from Task I?\n",
    "2. Is entropy a good metric for evaluating lexical diversity in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da43f721-1d6d-4840-8ca2-7b270299b884",
   "metadata": {},
   "source": [
    "# Part III: Document Similarity\n",
    "\n",
    "Throughout this course we will discuss the notion of *similarity* between texts and explore ways to measure it. This is a critical component of search and recommender systems. One such approach involves measuring how *close* two word distributions are using the notion divergence, which we discussed in the first lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a719ae9c-e5a8-45cb-9d1e-b2b40dfc3d16",
   "metadata": {},
   "source": [
    "### (10 pts) Task III\n",
    "\n",
    "Using the definition below, compute the KL-divergence, $K_{DL}$, between the word distributions in each category. This will result in a $K \\times K$ matrix of divergence values.\n",
    "\n",
    "$$ D_{KL}(P||Q) = \\sum_{i=1}^{N} P(x_{i}) \\log \\frac{P(x_{i})}{Q(x_{i})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304a543-1c5d-4408-b8b7-bc8d97e05912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53190e2c-903d-4cf4-90c1-f50571bdc825",
   "metadata": {},
   "source": [
    "### Explain this result\n",
    "\n",
    "1. How did you handle any differences in the support for P and Q? What about when Q(x) = 0?\n",
    "2. What does it mean for two distributions to have high or low divergence?\n",
    "3. Do these similarity scores make sense intuitively?\n",
    "4. Is the resultant $K \\times K$ matrix symmetric? Why is this the case?\n",
    "5. Is $D_{KL}$ a good measure of the similarity in this context?   "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f986a2461f5786d3cd1b7b30856ec5b277970b9a8cbc7da222a05b7f2c0b8370"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
