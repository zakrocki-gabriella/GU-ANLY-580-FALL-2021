{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Text Classification on the DBpedia14 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives:\n",
    "1. Build a Naive Bayes classification model from scratch\n",
    "2. Evaluate the performance of your model on the DBpedia14 dataset\n",
    "3. Train an off-the-shelf NB classifier and compare its performance to your implementation\n",
    "4. Train off-the-shelf implementations of the linear-SVM, RBF-kernel-SVM, and perceptron and compare their performance with the NB models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Reading\n",
    "\n",
    "1. https://arxiv.org/pdf/1811.12808.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80322c9083354b0eba51b71627c28828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=2142.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f415bd60fb4cd683f020f2939b6735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1288.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset d_bpedia14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /Users/gabriellazakrocki/.cache/huggingface/datasets/d_bpedia14/dbpedia_14/2.0.0/7f0577ea0f4397b6b89bfe5c5f2c6b1b420990a1fc5e8538c7ab4ec40e46fa3e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368da4f0281b4a518f39379285b1a3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset d_bpedia14 downloaded and prepared to /Users/gabriellazakrocki/.cache/huggingface/datasets/d_bpedia14/dbpedia_14/2.0.0/7f0577ea0f4397b6b89bfe5c5f2c6b1b420990a1fc5e8538c7ab4ec40e46fa3e. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "train_ds, test_ds = datasets.load_dataset('dbpedia_14', split=['train[:80%]', 'test[80%:]'])\n",
    "df_train: pd.DataFrame = train_ds.to_pandas()\n",
    "df_test: pd.DataFrame = test_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part I: Build your own Naive Bayes classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5 pts) Task I: Build a model from scratch\n",
    "Using your notes from lecture-02, implement a Naive Bayes model and train it on the DBpedia dataset. Also, feel free to use any text preprocessing you wish, such as the pipeline from Lab02. \n",
    "\n",
    "Below is a template class to help you think about the structure of this problem (feel free to design your own code if you like). It contains methods for each inference step in NB. It also has a classmethod that you could use to instantiate the class from a list of documents and a corresponding list of labels. Here we are suggesting you create a dictionary that maps each word to a unique $ith$ index in the $\\phi_{i,k}$ probabilty matrix, which you need to estimate. Because the labels are a set of 0-indexed integers, they naturally map to a unique position $\\mu_{k}$ (you should check this to make sure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayesModel:\n",
    "    \n",
    "    \"\"\"Multinomial NB model class template\"\"\"\n",
    "    \n",
    "    phi: np.ndarray # (N, K)\n",
    "    \n",
    "    mu: np.ndarray  # (K,)\n",
    "    \n",
    "    vocab: dict     # vocabulary map from word to row index in phi\n",
    "    \n",
    "    n_class: int    # number of classes\n",
    "    \n",
    "    \n",
    "    def __init__(vocabulary: dict, num_classes: int):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocabulary: {str: int} <- {word: index}\n",
    "        num_classes: Number of classes\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def from_preprocessed_data(cls, docs_list: List[str], labels_list: List[int]):\n",
    "        pass\n",
    "    \n",
    "    def estimate_mu(alpha: float = 1.):\n",
    "        \"\"\"\n",
    "        Estimate P(Y), the prior over labels\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: smoothing parameter\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def estimate_phi():\n",
    "        \"\"\"\n",
    "        Estimate phi, the N x K matrix \n",
    "        describing the probability of\n",
    "        the nth word in the kth class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: smoothing parameter\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict_label(text: str) -> int:\n",
    "        \"\"\"\n",
    "        Compute label given some input text\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text: raw input text\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int: corresponding to the predicted label\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Model performance evaluation\n",
    "\n",
    "Evaluating the performance of a classification model may seem as simple as computing an accuracy, and in some cases that is sufficient, but in general accuracy is not a reliable metric by itself. Typically we need to evaluate our model using several different metrics. \n",
    "\n",
    "One common issue is class imbalance, which is when the label distribution in the data varies far from uniform. In this case a high accuracy can be misleading because low frequency labels don't contribute equally to the score. More generally, this is one of the biggest drawbacks of using MLE in NLP: models tend to be much less sensitive to low probability labels than to higher probabilty labels. Later in this class we will explore models that learn by predicting words given their context, can you think of reasons why this can be problematic? Hint: remember Zipf's law?\n",
    "\n",
    "Another reason to use multiple evaluation methods is that it can help you better understand your data. Evaluating performance on individual classes often reveals problems with the data that would otherwise go unnoticed. For example, if you observe an abundance of misclassified data specific to only a few classes, chances are you have inconsistent labels for those classes in the training set. This is very common in 3rd party mechanical turk data, where quality can vary wildly.\n",
    "\n",
    "In this lab we will use three metrics and one visualization tool:\n",
    "\n",
    "1. [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision)\n",
    "2. [F1 score](https://en.wikipedia.org/wiki/F-score)\n",
    "3. [AUC ROC score](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "4. [The confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "The [metrics module](https://scikit-learn.org/stable/modules/model_evaluation.html) within sklearn provides support for nearly any evaluation metric that you will need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Compare your performance to an off-the-shelf NB classifier\n",
    "Open source implementations of your custom NB classifier from Part I already exist of course. One such implementation is [`sklearn.naive_bayes.MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) from the sklearn library. \n",
    "\n",
    "### (5 pts) Task II: NB model comparison\n",
    "Train this model on the same data and compare its performance with your model using the metrics from part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Compare NB to other classification models\n",
    "\n",
    "Now that we've built and validated our NB classifier, we want to evaluate other models on this task.\n",
    "\n",
    "### (5 pts) Task III: Evaluate the perceptron, SVM (linear), and SVM (RBF kernel)\n",
    "Train and evaluate the following models on this dataset, and compare them with the NB models.\n",
    "\n",
    "1. [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron)\n",
    "2. [Linear-SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
    "3. [RBF-Kernel-SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5 pts) Task IV: Select the best model\n",
    "\n",
    "1. Which model performed the best overall? \n",
    "2. What metric(s) influence this decision?\n",
    "3. Does the model that learns a non-linear decision boundary help?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
