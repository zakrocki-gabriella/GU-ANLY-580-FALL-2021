{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-09: DBpedia14 classification with BERT\n",
    "\n",
    "In Lab-03 we used linear text classification models (the perceptron, linear SVM, multinomial naive Bayes, softmax regression) on the DBpedia14 dataset. Recall that our best test accuracy was in the 92-93% range. Here we will use the BERT transformer model pretrained on vast amounts of text data to achieve ~99% test accuracy with minimal hyperparameter tuning.\n",
    "\n",
    "### Class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0: \"Company\",\n",
    "    1: \"EducationalInstitution\",\n",
    "    2: \"Artist\",\n",
    "    3: \"Athlete\",\n",
    "    4: \"OfficeHolder\",\n",
    "    5: \"MeanOfTransportation\",\n",
    "    6: \"Building\",\n",
    "    7: \"NaturalPlace\",\n",
    "    8: \"Village\",\n",
    "    9: \"Animal\",\n",
    "    10: \"Plant\",\n",
    "    11: \"Album\",\n",
    "    12: \"Film\",\n",
    "    13: \"WrittenWork\"\n",
    "}\n",
    "\n",
    "K = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab provides limited GPU time (be mindful when increasing this number)\n",
    "M = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNtj5tif0Jm5",
    "outputId": "71875ad9-7a63-425d-9f67-98797714dcf3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "df_train, df_test = datasets.load_dataset(\n",
    "    'dbpedia_14', \n",
    "    split=['train[:80%]', \n",
    "           'test[80%:]']\n",
    ")\n",
    "\n",
    "df_train = df_train.to_pandas().sample(frac=1).reset_index(drop=True)[:int(0.8 * M)]\n",
    "df_test = df_test.to_pandas().sample(frac=1).reset_index(drop=True)[:int(0.2 * M)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tL7neLwmM7J2"
   },
   "source": [
    "## (5 pts) Task 1: Analyze the data\n",
    "\n",
    "a. What is the distribution of the labels in this data? *Hint*: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html\n",
    "\n",
    "b. Compute the distribution of sentence length in the dataset\n",
    "\n",
    "c. Are there any differences between the train and test sets? If so make any necessary changes to the train/test sets such that they look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJlkSeUmRMQ6",
    "tags": []
   },
   "source": [
    "## (10 pts) Task 2: Train BERT on DBpedia14 using the provided code\n",
    "\n",
    "Here you just need to run the cells below on a GPU. To do this we are going to use Google CoLab, which provides free (but limited) use of hosted GPU instances.\n",
    "\n",
    "Before doing this, take a look at the leaderboard for DBpedia14: https://paperswithcode.com/dataset/dbpedia. You'll find various transformers (including BERT) at the top. Here we will use a smaller version of BERT called DistilBERT which reduces the computational overhead with only marginal performance degradation, and only two training epochs over an abridged version of the dataset.\n",
    "\n",
    "How does DistilBERT perform relative to the linear text classifiers from Lab 03?\n",
    "\n",
    "### Google CoLab\n",
    "\n",
    "To complete tasks 2 & 3, you will need to login to a Google account and then go here: https://colab.research.google.com. Then do the following:\n",
    "\n",
    "1. End any existing CoLab sessions that you have running\n",
    "\n",
    "2. Click on the `Upload` tab and upload this file\n",
    "\n",
    "3. Click on `Runtime -> Change runtime type` and select `GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart runtime\n",
    "\n",
    "After running the cell above, click `Runtime -> Restart runtime`. This only needs to be done one time per session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LU0ogJe5sG3d"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "lr_init = 1e-5\n",
    "max_len = 256\n",
    "warmup_steps = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batched inputs using Huggingface's DistilBERT tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3lKXjp1b0UL"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "def batch_data(data, bsize):\n",
    "    batches = []\n",
    "    sentences = data['content'].tolist()\n",
    "    labels = data['label'].tolist()\n",
    "    for i in range(0, len(sentences), bsize):\n",
    "        s = sentences[i: i + bsize]\n",
    "        Y = labels[i: i + bsize]\n",
    "        X = tokenizer.batch_encode_plus(\n",
    "            s, max_length=max_len, padding='longest', truncation=True,\n",
    "            return_attention_mask=True, return_token_type_ids=False)\n",
    "        batches.append((X, Y, s))\n",
    "    return batches\n",
    "\n",
    "train_batches = batch_data(df_train, bsize=batch_size)\n",
    "test_batches = batch_data(df_test, bsize=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained Distilbert model\n",
    "\n",
    "This cell uses the `DistilBertForSequenceClassification` class, which is a convenience wrapper that places a classification head onto the DistilBert language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBWQH1YgMjs3",
    "outputId": "56913153-1579-4ed7-e3d2-7c85630dc796"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, \\\n",
    "  AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-cased', \n",
    "    num_labels=K, \n",
    "    output_hidden_states=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0') # GPU\n",
    "else:\n",
    "    device = torch.device('cpu') # CPU\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr_init)\n",
    "lr = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=len(train_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "*Note: This should take about 20 minutes on CoLabs K80 GPUs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "def runner(batches, desc: str, train=True):\n",
    "    \n",
    "    grad_mode = torch.enable_grad if train else torch.no_grad\n",
    "    preds = []\n",
    "    \n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    for epoch in range(epochs if train else 1):\n",
    "        \n",
    "        acc = load_metric(\"accuracy\", keep_in_memory=True)\n",
    "        f1 = load_metric(\"f1\", keep_in_memory=True)\n",
    "        cumloss = 0.0\n",
    "        embeds = []\n",
    "        \n",
    "        with tqdm(total=len(batches)) as bar:\n",
    "\n",
    "            for i, batch in enumerate(batches):\n",
    "                X, Y, _ = batch\n",
    "                inputs = torch.tensor(X['input_ids'], device=device)\n",
    "                attmsk = torch.tensor(X['attention_mask'], device=device)\n",
    "                labels = torch.tensor(Y, device=device)\n",
    "                batch = {'input_ids': inputs,\n",
    "                         'attention_mask': attmsk,\n",
    "                         'labels': labels}\n",
    "                with grad_mode():\n",
    "                    outputs = model(**batch)\n",
    "                    embeds.append(outputs[-1][1][:, 0, :].squeeze().detach().cpu())\n",
    "                    loss = outputs.loss\n",
    "                    if train:\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        lr.step()\n",
    "                        optimizer.zero_grad()\n",
    "                    logits = outputs.logits\n",
    "                    Yhat = torch.argmax(logits, dim=-1)\n",
    "                    preds.append(Yhat)\n",
    "                    cumloss += loss.clone().detach().cpu().item()\n",
    "                    acc.add_batch(predictions=Yhat, references=Y)\n",
    "                    f1.add_batch(predictions=Yhat, references=Y)\n",
    "\n",
    "                bar.update(1)\n",
    "            bar.set_description('epoch: %s, %s loss: %.5f, f1-score: %.5f, accuracy: %.5f' %\n",
    "                                (epoch + 1, desc,\n",
    "                                 cumloss / (i + 1),\n",
    "                                 f1.compute(average=\"macro\")['f1'],\n",
    "                                 acc.compute()['accuracy']))\n",
    "                \n",
    "    embeds = torch.cat(embeds, dim=0)\n",
    "        \n",
    "    return preds, embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZFo7acOcMkV",
    "outputId": "2e17b728-6602-4b66-aad2-6714efb6aad9"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "runner(train_batches, 'train');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16sTsM5GOvji",
    "outputId": "b7183598-edd4-40cd-e439-fa63968a7616"
   },
   "outputs": [],
   "source": [
    "# Evaluate training set\n",
    "runner(train_batches, 'train', train=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMHr99DreMUJ",
    "outputId": "c1f89710-840b-44e9-e7bc-0836001855d9"
   },
   "outputs": [],
   "source": [
    "# Evaluate test set\n",
    "runner(test_batches, 'test', train=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7IM0hhosOf6"
   },
   "source": [
    "## (5 pts) Task 3: test your model on new string\n",
    "\n",
    "Make up (or find) a document that maps to one of the 14 classes in DBpedia14. Is the model's prediction in agreement with your label assignment? The cell below contains some helper code to get you started. To compute the model prediction, reuse the `batch_data()` and `runner()` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "xai30ZLxCuQ0",
    "outputId": "21cf7432-d06f-476d-fedc-0a579f24f707"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "example_text = \"\"\"Her is a 2013 American science-fiction romantic drama film written, \n",
    "directed, and produced by Spike Jonze. It marks Jonze's solo screenwriting debut. \n",
    "The film follows Theodore Twombly (Joaquin Phoenix), a man who develops a relationship \n",
    "with Samantha (Scarlett Johansson), an artificially intelligent virtual assistant \n",
    "personified through a female voice.\"\"\"\n",
    "\n",
    "example_label = 12 # \"film\"\n",
    "\n",
    "df = pd.DataFrame({'content': [example_text], 'label': example_label, 'title': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mNzs0l9O4V-"
   },
   "source": [
    "## (5 pts extra credit) Task 4: Extract the document embeddings and visualize them in Tensorboard Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSV helper functions from Lecture 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_matrix(matrix, fpath):\n",
    "    D1, D2 = matrix.shape\n",
    "    tsv = \"\"\n",
    "    for i in range(D1):\n",
    "        for j in range(D2):\n",
    "            tsv += str(matrix[i, j]) + '\\t'\n",
    "        tsv = tsv.strip('\\t') + '\\n'\n",
    "    tsv = tsv.strip('\\n')\n",
    "    with open(fpath, \"w\") as fd:\n",
    "        fd.write(tsv)\n",
    "\n",
    "def save_docs(docs, labels, fpath):\n",
    "    tsv = \"document\\tlabel\\n\"\n",
    "    for doc, label in zip(docs, labels):\n",
    "        tsv += doc.lower().strip() + '\\t' + label + '\\n'\n",
    "    tsv = tsv.strip('\\n')\n",
    "    with open(fpath, \"w\") as fd:\n",
    "        fd.write(tsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize embeddings\n",
    "\n",
    "The embeddings and text can be saved in tsv format and uploaded here  https://projector.tensorflow.org/ for visualization. Choose the tSNE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CL Digits Work Sample - Machine Learning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
