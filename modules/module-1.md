# Module 1: Text Classification

Text classification is the process of mapping unstructured text onto a set of discrete categories. On its surface, classification might seem like one small sliver of the universe of tasks we might want to perform, but as we'll learn, classification is in many ways the foundational component of modern NLP systems.

### Module Topics

1. Core concepts in linear algebra, probability theory, information theory, and optimization that are used in text classification

2. Maximum likelihood estimation (MLE)

3. Text normalization methods based on stemming, lemmatization, text cleaning with regular expressions, and how to implement these methods using Spacy.

4. The Bag-of-Words (BOW) feature representation of text

5. Linear classification modeling techniques such as Naive Bayes, the perceptron, and linear SVMs.

### Outcomes

You will leave this module with the following skills / knowledge:

1. Be able to perform basic text preprocessing and cleaning using regular expressions and Spacy.

2. Know basic concepts in probability theory such as marginal, joint, and conditional distributions, the chain rule, independence and conditional independence, and Bayes Rule.

3. Understand the differences between generative and discriminative modeling and how this relates back to (2).

4. Understand basic concepts from information theory such as self information, entropy, KL divergence, and cross entropy.

5. Understand the principle of maximum likelihood, the assumptions it makes about a set of observed data, how it is used to estimate the parameters of a model, and how/why many classification problems in machine learning are solved through cross-entropy minimization.

6. Understand how Naive Bayes is applied to text classification, the assumptions that it makes about the data, why smoothing is important, and be able to implement NB both from scratch and using open source packages. 

7. Be familar with linear decision boundary learning techniques, understand the idea behind maximum margin classifiers, and be able to implement the perceptron and SVMs using open source packages.

#
### Due Dates

- Quiz 1 (covers course policy, non-technical)
    - Section 1: Sep 08 11:59pm EST
    - Section 2: Sep 08 11:59pm EST

- Quiz 2
    - Section 1: Sep 19 11:59pm EST
    - Section 2: Sep 15 11:59pm EST

- Assignment 1
    - Section 1: Sep 26 11:59pm EST
    - Section 2: Sep 22 11:59pm EST

- Lab 1
    - Section 1: Sep 08 11:59pm EST
    - Section 2: Sep 08 11:59pm EST

- Labs 2,3
    - Section 1: Dec 08 11:59pm EST
    - Section 2: Dec 08 11:59pm EST
