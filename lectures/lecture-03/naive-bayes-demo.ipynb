{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c2af33",
   "metadata": {},
   "source": [
    "# Naive Bayes Example\n",
    "\n",
    "Build a Naive Bayes classifier with smoothing using the training data below to predict the label of the test example.\n",
    "\n",
    "| Set | Label | Document | # |\n",
    "| --- | --- | --- | --- |\n",
    "| Train | - | bad experience | 1 |\n",
    "| Train | - | lacked ambiance | 2 |\n",
    "| Train | - | food was not great | 3 |\n",
    "| Train | + | amazing experience | 4 |\n",
    "| Train | + | delicious food | 5 |\n",
    "| Test  | ? | food was amazing, great ambiance | 6 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a8e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_docs = [\"bad experience\", \"lacked ambiance\", \"food was not great\", \"amazing experience\", \"delicious food\"]\n",
    "# Labels âˆˆ {0, 1} <=> {-,+}\n",
    "ytr = np.array([0, 0, 0, 1, 1])\n",
    "test_doc = \"food was amazing great ambiance\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a47b3",
   "metadata": {},
   "source": [
    "### Construct Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5af839",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = {word: idx for idx, word in enumerate(set(\" \".join(train_docs).split(\" \")))}\n",
    "featurizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c65640",
   "metadata": {},
   "source": [
    "### Construct BOW features w/smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641858f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = np.zeros(shape=(len(train_docs), len(featurizer)))\n",
    "for i, doc in enumerate(train_docs):\n",
    "    for word in doc.split(\" \"):\n",
    "        j = featurizer[word]\n",
    "        Xtr[i, j] += 1\n",
    "\n",
    "Xte = np.zeros(shape=(len(featurizer)))\n",
    "for word in test_doc.split(\" \"):\n",
    "    j = featurizer[word]\n",
    "    Xte[j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f1e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(train_docs)\n",
    "N = len(featurizer)\n",
    "K = len(set(ytr))\n",
    "M, N, K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087358f8",
   "metadata": {},
   "source": [
    "### Parameter estimation for $P(y ; \\boldsymbol{\\mu})$, where $\\boldsymbol{\\mu} \\in \\mathbb{R}^{K}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_hat = np.array([sum(ytr == idx) / M for idx in range(K)])\n",
    "mu_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89e6ecf",
   "metadata": {},
   "source": [
    "### Parameter estimation for $P(\\boldsymbol{x} | y; \\boldsymbol{\\phi})$, where $\\boldsymbol{\\phi} \\in \\mathbb{R}^{K \\times N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0849862",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_by_class = {\n",
    "    k: np.sum(Xtr[np.where(ytr == k)])\n",
    "for k in range(K)}\n",
    "word_count_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing constant (this can be any real number > 0)\n",
    "alpha = 1.0\n",
    "phi_hat = np.zeros(shape=(K, N))\n",
    "for word, j in featurizer.items():\n",
    "    for k in range(K):\n",
    "        num_word_j_class_k = sum(np.squeeze(Xtr[np.where(ytr == k), j]))\n",
    "        phi_hat[k, j] = (alpha + num_word_j_class_k) / (alpha * N + word_count_by_class[k])\n",
    "np.sum(phi_hat, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371ff61",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Predict label by computing the argmax of the log likelihood over class labels via Bayes Rule: $ P(y | \\boldsymbol{x}_{te}) \\propto  \\boldsymbol{x}_{te} (log \\boldsymbol{\\phi})^{T} + log \\boldsymbol{\\mu}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8cf36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_y_given_Xte = Xte.dot(np.log(phi_hat).T) + np.log(mu_hat)\n",
    "p_y_given_Xte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32748a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "yte_hat = np.argmax(p_y_given_Xte)\n",
    "yte_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f9a2e",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Clearly our NB model has misclassified the test example. There a couple of things going on here: \n",
    "1. $M$ is small (5), and there is class imbalance as reflected in $\\hat{\\boldsymbol{\\mu}} = [0.6, 0.4]$. Coupling this with the structure of the likelihood function for $P(y | \\boldsymbol{x})$, we see that the log prior term can have a pretty big effect on the result.\n",
    "2. The contextual meaning of words is largely lost with BOW features. Here, the word *great* is associated with negative sentiment in the training set, while used to express positive sentiment in the test example.\n",
    "3. The log likelihood values are very close to each other. In this case there are very few examples and very few words, and therefore adding a constant 1 to each entry in $X$ pushes these probabilities together. Try adjusting the `alpha` parameter to be big (e.g., 2) and small (e.g., 0.01)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
