{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e40ef4c-0c8a-4ef4-b213-61a6f9bdce96",
   "metadata": {},
   "source": [
    "# Neural networks for text classification\n",
    "\n",
    "This demo provides very basic Numpy implementations of the following:\n",
    "\n",
    "1. A feed forward NN represented as a doubly linked list\n",
    "2. A text classifier that uses a CBOW feature representation\n",
    "3. Autograd\n",
    "\n",
    "*Note: This demo conveys some of the underlying constructs used in deep learning packages, not their implementations of those constructs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529121b5-f844-452d-842e-0c881a7f0212",
   "metadata": {},
   "source": [
    "### Load DBPedia14 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0495d64a-b5ea-4807-b417-aa2d08e9d46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset d_bpedia14 (/Users/chris/.cache/huggingface/datasets/d_bpedia14/dbpedia_14/2.0.0/7f0577ea0f4397b6b89bfe5c5f2c6b1b420990a1fc5e8538c7ab4ec40e46fa3e)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "M = 2500\n",
    "\n",
    "df = datasets.load_dataset(\n",
    "    'dbpedia_14', \n",
    "    split=['train[:100%]', \n",
    "           'test[100%:]']\n",
    ")[0].to_pandas().sample(frac=1).reset_index(drop=True)[:M]\n",
    "\n",
    "K = len(set(df.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d233247-1378-4de9-b154-cf65b8b193e5",
   "metadata": {},
   "source": [
    "### Spacy preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9d75d2-26e8-4e76-be4b-cc76db0b6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "pipeline_name = 'DBPedia14'\n",
    "\n",
    "\n",
    "def camel_case_split(str):\n",
    "    return \" \".join([wrd for wrd in re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', str)])\n",
    "\n",
    "\n",
    "@Language.component(pipeline_name)\n",
    "def preprocess(doc):\n",
    "    doc = [token for token in doc if not token.is_punct]\n",
    "    # doc = [token for token in doc if not token.is_stop]\n",
    "    doc = [token.text.lower().strip() for token in doc]\n",
    "    doc = [token for token in doc if 0 < len(token) <= 12]\n",
    "    return \" \".join(doc)\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \n",
    "    # http://emailregex.com/\n",
    "    email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)\n",
    "    *|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]\n",
    "    |\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9]\n",
    "    (?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}\n",
    "    (?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:\n",
    "    (?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "    # replace = [ (pattern-to-replace, replacement),  ...]\n",
    "    replace = [\n",
    "        (\"<[^>]*>\", \" \"),\n",
    "        (email_re, \" \"),                           # Matches emails\n",
    "        (r\"(?<=\\d),(?=\\d)\", \"\"),                   # Remove commas in numbers\n",
    "        (r\"\\d+\", \" \"),                             # Map digits to special token <numbr>\n",
    "        (r\"[*\\^\\.$&@<>,\\-/+{|}=?#:;'\\\"\\[\\]]\", \"\"), # Punctuation and other junk\n",
    "        (r\"[\\n\\t\\r]\", \" \"),                        # Removes newlines, tabs, creturn\n",
    "        (r\"[^\\x00-\\x7F]+\", \"\"),                    # Removes non-ascii chars\n",
    "        (r\"\\\\+\", \" \"),                             # Removes double-backslashs\n",
    "        (r\"\\s+n\\s+\", \" \"),                         # 'n' leftover from \\\\n\n",
    "        (r\"\\s+\", \" \")                              # Strips extra whitespace\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipeline = spacy.load('en_core_web_sm')\n",
    "        self.pipeline.add_pipe(pipeline_name);\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.transform(*args, **kwargs)\n",
    "\n",
    "    def transform(self, doc: str):\n",
    "        for repl in self.replace:\n",
    "            doc = re.sub(repl[0], repl[1], doc)\n",
    "        doc = camel_case_split(doc)\n",
    "        return self.pipeline(doc)\n",
    "    \n",
    "pipeline = Pipeline();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03199f-15c8-434c-a5f8-03863a8183fc",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0617ac12-e428-4960-9d59-cf70a25aaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/2500 [00:00<?, ?it/s]/Users/chris/Documents/GU/GU-ANLY-580/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2500/2500 [00:15<00:00, 163.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with tqdm(total=M) as bar:\n",
    "    for i, content in enumerate(df.content.tolist()):\n",
    "        df.content[i] = pipeline(content)\n",
    "        bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba26ee-1be0-43b1-b2e1-6766316409cb",
   "metadata": {},
   "source": [
    "### Featurizer\n",
    "\n",
    "Below we will build a BOW featurizer, and use it as both a CBOW featurizer and as an embedding lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f68c0fd-d26e-4c29-8ea1-19f5f0bcdb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11697"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vocab_size = len(set(\" \".join(df.content.tolist()).split(\" \")))\n",
    "\n",
    "featurizer = CountVectorizer(max_features=vocab_size, stop_words=None)\n",
    "featurizer.fit(df.content.tolist());\n",
    "featurizer.get_idx = {word: idx for idx, word in enumerate(featurizer.get_feature_names())}\n",
    "featurizer.get_word = {idx: word for idx, word in enumerate(featurizer.get_feature_names())}\n",
    "\n",
    "N = len(featurizer.get_idx)\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df714e2-fade-4db8-b994-dc2c35f6e8dd",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89e898c-c13b-4f89-8ce0-2eb4816f57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "Y = df.label.to_numpy()\n",
    "\n",
    "# One-hot features (just word index pointers)\n",
    "X = [[featurizer.get_idx[word] for word in doc.split(\" \") \n",
    "            if featurizer.get_idx.get(word) is not None] for doc in df.content]\n",
    "\n",
    "# BOW features\n",
    "X_bow = np.zeros(shape=[M, N], dtype=int)\n",
    "for i, seq in enumerate(X):\n",
    "    for idx in seq:\n",
    "        X_bow[i, idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0977cd1-e36e-45fb-ac87-04c26fd18352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jazz',\n",
       " 'joe',\n",
       " 'jones',\n",
       " 'lewis',\n",
       " 'mel',\n",
       " 'orchestra',\n",
       " 'presenting',\n",
       " 'records',\n",
       " 'solid',\n",
       " 'state',\n",
       " 'thad',\n",
       " 'williams']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[featurizer.get_word[idx] for idx, c in enumerate(X_bow[0]) if c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab7233-c61c-438e-bc36-bcf3a50f55a1",
   "metadata": {},
   "source": [
    "## Neural network as a graph\n",
    "\n",
    "Deep learning packages implement neural networks as a graph, where each node represents a layer within the network, and each edge represents a transformation that maps one layer onto the another. The two most important components of any deep learning library are (i) efficient implementations of the various transformations used in deep learning (graph edges) that are optimized for various hardware (x86, ARM, NVIDIA GPUS, TPUs etc..), and (ii) automatic differentiation (autograd) which enables users to use the package constructs (linear transformations, activations, loss functions, optimizers etc.) without ever having to implement (or even think about) backpropogation. Below is a (very rudimentary and fairly inefficient) implementation of the basic computational units that are needed to implement a feed forward network.\n",
    "\n",
    "*Note: There are two numpy multiplication operations being used here: 1) `@` for matrix multiplication, and 2) `*` for the element-wise (aka Hadamard, aka Schur) product*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4beefbf-5a60-476f-88be-79cfe34236fa",
   "metadata": {},
   "source": [
    "### Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2a3aee-913b-41f0-9d75-3574ba307484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(P, y):\n",
    "    ce = np.sum(-y * np.log(P), axis=1)\n",
    "    return np.mean(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40c73b-9d70-4e29-95de-a93e458f6888",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1890429-529e-4333-b825-70d6dbe04acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(P, Y):\n",
    "    Y_hat = np.argmax(P, axis=1)\n",
    "    acc = accuracy_score(Y, Y_hat)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879681c5-d4d5-4223-8b6d-9d425066d557",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfb1f2e8-c49e-4955-9a36-a8c6275b06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    partition = np.sum(Z, axis=1, keepdims=True)\n",
    "    return Z / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424c821-7d6c-4e42-99e4-292f387fb162",
   "metadata": {},
   "source": [
    "### Weight initializer\n",
    "\n",
    "Weight initialization is crucially important in deep learning, if not done properly your network will just diverge after the first gradient update. There are many options here, but it primarily boils down to what distribution do you want your intial weight values to be drawn from. A great choice is *Xavier* initialization, named after it's author Xavier Glorot, which draws weights from a normal distribution $W_{ij} \\sim N(\\mu=0, \\sigma^2=n)$, where $W \\in \\mathbb{R}^{n \\times \\cdot}$. For large weight matrices, this is a means to control the variance of the output, $z$ at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af109dfe-af27-44d9-9a23-8000a7d6f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(*dims):\n",
    "    \"\"\" Xavier initialization \"\"\"\n",
    "    return (1 / np.sqrt(dims[0])) * np.random.randn(*dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beff7c6-a128-4d83-ad46-e8b426f591a8",
   "metadata": {},
   "source": [
    "### Dropout module\n",
    "\n",
    "Dropout is one of the most important regularization techniques in deep learning. In each activation layer during training, a random subset (chosen with probability `prob`) of the activations are masked out (zeroed). On an intuitive and somewhat handwavy level, you can think of this as a way to prevent the network over relying on on a small subset of the pathways through the network, and helps force all of the nodes, and therefore weights, to be utilized (quasi) equally. By doing this, we are scaling the values of the output layer; in order to keep the distribution over each layer consistent during the training and test phases, this means we must apply a scaling factor either during test time ($\\frac{1}{prob}$, traditional dropout), or during training ($\\frac{1}{1-prob}$, inverted dropout). Inverted dropout is the preferred method as it incurs no runtime cost during inference, while only adding marginal runtime overhead during training (remember, the vast majority of the computation during training is from backpropogation). This is an implementation of inverted dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e55c06b5-8bda-4cd2-a23f-5787f873a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dropout(a: np.ndarray, prob: float):\n",
    "    mask = np.random.choice([0.0, 1.0], size=a.shape, p=[prob, 1 - prob])\n",
    "    a *= mask / (1 - prob)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3551ab-7aed-4d6c-b0a1-b4c411ea0a46",
   "metadata": {},
   "source": [
    "### Graph node\n",
    "\n",
    "The doubly linked list is a natural data structure for implementing networks. Below we make a class called `Node` which will serve as the base class for the various layer types in our network. In this implementation, each `Node` will contain three things: (i) memory to store a layer (`Node.out`), (ii) a method to map the parent node's output layer to its output (`Node.out = Node.forward(Node.last.out)`), (iii) a method to backpropogate gradients from its child node to its parent node (`Node.error = Node.backward(Node.next.error)`), and (iv) a set of methods to perform gradient updates to the parameters that it owns (`Node.update()`, `Node.zero_grad()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67a377e7-7c63-4598-beb0-495c4371e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    \"\"\" \n",
    "    Node in computational graph (doubly linked list)\n",
    "    Note: not to be confused with a node in a network layer\n",
    "    \n",
    "    Attributes:\n",
    "    last: Node        # Parent\n",
    "    next: Node        # Child\n",
    "    out: np.ndarray   # Output layer\n",
    "    error: np.ndarray # dCE/d`out`\n",
    "    dim: int          # layer dimension\n",
    "    lr: float         # learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, last, dim: int, lr: float = None):\n",
    "        self.last = last\n",
    "        if self.last:\n",
    "            self.last.next = self\n",
    "        self.next = None\n",
    "        self.out = None\n",
    "        self.error = None\n",
    "        self.dim = dim\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, dropout: bool = False):\n",
    "        pass\n",
    "    \n",
    "    def backward(self):\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass\n",
    "    \n",
    "    def update(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36554d1-7855-48aa-ba81-24a3a0567ff4",
   "metadata": {},
   "source": [
    "### Input embedding node\n",
    "\n",
    "This is an embedding lookup, which will be the first layer in our network. The forward method accepts a batch of BOW features and computes their inner-product with an embedding lookup table. This is an implementation of the continuous *bag-of-features*, in which we we simply sum the embedding for each word in the the BOW input, weighted by their frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "126c801f-81e9-490d-b898-747b73baaea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(Node):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, lr, wt_decay):\n",
    "        super().__init__(None, embedding_dim, lr)\n",
    "        self.W = weight_init(embedding_dim, vocab_size)\n",
    "        self.b = weight_init(1, self.dim)\n",
    "        self.wt_decay = wt_decay\n",
    "    \n",
    "    def forward(self, X, dropout):\n",
    "        self.input = X\n",
    "        self.out = X @ self.W.T + self.b\n",
    "        if self.next:\n",
    "            self.next.forward(dropout)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.W_grad = np.zeros_like(self.W)\n",
    "        self.b_grad = np.zeros_like(self.b)\n",
    "    \n",
    "    def update(self):\n",
    "        n = self.out.shape[0]\n",
    "        l1_grad = self.wt_decay * np.sign(self.W)\n",
    "        l2_grad = self.wt_decay * self.W\n",
    "        self.W -= self.lr * ((1 / n) * self.W_grad + l1_grad + l2_grad)\n",
    "        self.b -= self.lr * (1 / n) * self.b_grad\n",
    "    \n",
    "    def backward(self, error=None):\n",
    "        if error is None:\n",
    "            error = self.next.error\n",
    "        dW = self.input\n",
    "        db = 1\n",
    "        self.W_grad = error.T @ dW\n",
    "        self.b_grad = np.sum(error, axis=0) * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4032844a-d090-4ece-96d7-c7f2c0d22a18",
   "metadata": {},
   "source": [
    "### Leaky relu activation\n",
    "\n",
    "The leaky relu (rectified linear unit) is a popular activation function (non-linearity).\n",
    "\n",
    "$LReLU(z) = \\max(z, \\alpha z) \\quad \\text{where} \\quad \\alpha \\in (0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac580b06-9dc9-4539-a381-7d7dfd00fb75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LReLU(Node):\n",
    "    \n",
    "    def __init__(self, last, alpha=0.001, dropout=0.5):\n",
    "        super().__init__(last, last.dim)\n",
    "        self.alpha = alpha\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, dropout):\n",
    "        self.out = np.where(self.last.out >= 0, \n",
    "                            self.last.out, \n",
    "                            self.alpha * self.last.out)\n",
    "        if dropout:\n",
    "            self.out = apply_dropout(self.out, prob=self.dropout)\n",
    "        self.next.forward(dropout)\n",
    "        \n",
    "    def update(self):\n",
    "        self.last.update()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.last.zero_grad()\n",
    "        \n",
    "    def backward(self):\n",
    "        da = np.where(self.last.out >= 0, 1, self.alpha)\n",
    "        self.error = self.next.error * da\n",
    "        self.last.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f68b62d-35c9-49b0-9446-f2dda8ebcaef",
   "metadata": {},
   "source": [
    "### Note on the *forward* and *backward* pass\n",
    "\n",
    "In the above cell you'll note that the `.forward()` method contains a call to `Node.next.forward()`; this is what allows data to flow from input layer to output layer. Also notice that the `.backward()`, `.update()`, and `.zero_grad()` methods all contain calls to either/both its child (`Node.next`) or parent (`Node.last`) along with code that computes gradents and updates parameter weights; this is very basic implementation of *autograd*. These features allow us to simply call `head_node.forward(some_input)` to compute the *forward pass*, and `tail_node.backward()` to compute the *backward pass* in the training loop at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc122a96-6c52-44df-b73d-9d31e35741d4",
   "metadata": {},
   "source": [
    "### Linear node\n",
    "\n",
    "This is the *linear* unit, sometimes referred to as a `dense` layer. It's just a batched implementation of $XW^T + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ad8809b-6c92-4013-b880-fa69fdcbf663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    \n",
    "    def __init__(self, last, dim: int, lr: float, wt_decay: float):\n",
    "        super().__init__(last, dim, lr)\n",
    "        self.W = weight_init(self.dim, self.last.dim)\n",
    "        self.b = weight_init(1, self.dim)\n",
    "        self.wt_decay = wt_decay\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.W_grad = np.zeros_like(self.W)\n",
    "        self.b_grad = np.zeros_like(self.b)\n",
    "        self.error = None\n",
    "        self.last.zero_grad()\n",
    "    \n",
    "    def update(self):\n",
    "        n = self.out.shape[0]\n",
    "        l2_grad = self.wt_decay * self.W\n",
    "        self.W -= self.lr * ((1 / n) * self.W_grad + l2_grad)\n",
    "        self.b -= self.lr * (1 / n) * self.b_grad\n",
    "        self.last.update()\n",
    "    \n",
    "    def forward(self, dropout):\n",
    "        self.out = self.last.out @ self.W.T + self.b\n",
    "        if self.next:\n",
    "            self.next.forward(dropout=dropout)\n",
    "        \n",
    "    def backward(self, error=None):\n",
    "        if error is None:\n",
    "            error = self.next.error\n",
    "        dW = self.last.out\n",
    "        db = 1\n",
    "        self.W_grad = error.T @ dW\n",
    "        self.b_grad = np.sum(error, axis=0) * db\n",
    "        self.error = error @ self.W\n",
    "        self.last.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9b3be6-73eb-437e-86c3-1bf157b664d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Network wrapper\n",
    "\n",
    "This is a wrapper around our doubly linked list, it allows us to define one object for interacting with the network rather than having to keep track of the *head* and *tail* nodes of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55a12444-15de-49d9-a87d-d63b57bf096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    input: Node\n",
    "    last: Node\n",
    "    \n",
    "    def set_lr(lr):\n",
    "        node = self.input\n",
    "        while node is not None:\n",
    "            node.lr = lr\n",
    "            node = node.next\n",
    "            \n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self.last.lr\n",
    "        \n",
    "    @property\n",
    "    def out(self):\n",
    "        return self.last.out\n",
    "    \n",
    "    def forward(self, x, dropout=False):\n",
    "        self.input.forward(x, dropout=dropout)\n",
    "    \n",
    "    def backward(self, loss):\n",
    "        self.last.backward(loss)\n",
    "        \n",
    "    def update(self):\n",
    "        self.last.update()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.last.zero_grad()\n",
    "        \n",
    "        \n",
    "class ANN(Network):\n",
    "    def __init__(self, vocab_size, layer_size, output_dim, num_layers, lr, wt_decay, dropout):\n",
    "        super().__init__()\n",
    "        self.input = InputEmbedding(vocab_size, layer_size, lr, wt_decay)\n",
    "        self.last = LReLU(self.input, dropout=dropout)\n",
    "        for i in range(num_layers - 1):\n",
    "            z = Linear(self.last, layer_size, lr, wt_decay)\n",
    "            self.last = LReLU(z, dropout=dropout)\n",
    "        self.last = Linear(self.last, output_dim, lr, wt_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afcf432-6efd-4174-87b2-8f8a9c5679c5",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3478014-45bf-427f-84b1-e1b39b2c9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lr = 0.0005\n",
    "wt_decay = 1e-7\n",
    "dropout = 0.0\n",
    "layer_size = 15\n",
    "layers = 2\n",
    "batch_size = 10\n",
    "epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83077fe8-f879-40cc-8e0b-f92f6ca4ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 250, CE: 0.0062, Acc: 0.9992: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62500/62500 [04:07<00:00, 252.78it/s]\n"
     ]
    }
   ],
   "source": [
    "net = ANN(N, layer_size, K, layers, lr=0.02, wt_decay=wt_decay, dropout=dropout)\n",
    "\n",
    "one_hot_labels = np.eye(K)\n",
    "\n",
    "shuffle_idx = np.arange(M)\n",
    "\n",
    "with tqdm(total=epochs * (M // batch_size)) as bar:    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Randomize data ordering\n",
    "        np.random.shuffle(shuffle_idx)\n",
    "        X_shfd = X_bow[shuffle_idx]\n",
    "        Y_shfd = Y[shuffle_idx]\n",
    "        \n",
    "        for i in range(0, M, batch_size):\n",
    "            \n",
    "            # Get batch of x,y pairs\n",
    "            x = X_shfd[i: i + batch_size]\n",
    "            y = Y_shfd[i: i + batch_size]\n",
    "            \n",
    "            # Skip empty slices\n",
    "            if not x.shape[0]:\n",
    "                continue\n",
    "            \n",
    "            # Forward pass\n",
    "            net.forward(x, dropout=True)\n",
    "            \n",
    "            # Compute softmax from net.out\n",
    "            P = softmax(net.out)\n",
    "            \n",
    "            # Backward pass\n",
    "            # Note: From lecture 05, remember that the derivative of the\n",
    "            # cross entropy loss w.r.t. the input (z) to the softmax P(z)\n",
    "            # is dCE/dz = P(z) - y.\n",
    "            net.backward(P - one_hot_labels[y])\n",
    "            \n",
    "            # Apply gradient updates\n",
    "            net.update()\n",
    "            \n",
    "            # Zero the stored gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            bar.update(1)\n",
    "        \n",
    "        # Learning rate annealing\n",
    "        new_lr = max(min_lr, 0.9 * net.lr) \n",
    "        \n",
    "        # Evaluate performance on training set w/o dropout\n",
    "        net.forward(X_bow)\n",
    "        P = softmax(net.out)\n",
    "        ce_tr = cross_entropy(P, one_hot_labels[Y])\n",
    "        acc_tr = compute_metrics(P, Y)\n",
    "        \n",
    "        bar.set_description(\"Epoch: %d, CE: %.4f, Acc: %.4f\" % (epoch + 1, ce_tr, acc_tr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
