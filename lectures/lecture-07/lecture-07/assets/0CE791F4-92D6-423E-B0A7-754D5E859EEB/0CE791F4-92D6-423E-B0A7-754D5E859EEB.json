{"assets":{"2AF74057B727D5E2D2870400A74FF722":{"type":"texture","index":0,"assetRequest":{"type":"slide","state":"contents","slide":"none"},"url":{"native":"assets\/0CE791F4-92D6-423E-B0A7-754D5E859EEB.pdf"},"width":1920,"height":1080},"27D0748305D7AC77AB5619FD63ED80C9":{"type":"texture","index":1,"assetRequest":{"type":"slide","state":"contents","slide":"none"},"url":{"native":"assets\/0CE791F4-92D6-423E-B0A7-754D5E859EEB.pdf"},"width":1920,"height":1080}},"events":[{"effects":[{"beginTime":0,"baseLayer":{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"objectID":"0","layers":[{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,-0.00035007912466775983,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"layers":[{"animations":[],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"27D0748305D7AC77AB5619FD63ED80C9"},{"animations":[{"additive":false,"timeOffset":0,"beginTime":0,"from":{"scalar":false},"repeatCount":0,"fillMode":"both","duration":0.01,"autoreverses":false,"property":"hidden","to":{"scalar":true},"removedOnCompletion":false}],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"2AF74057B727D5E2D2870400A74FF722"}]}]},"effects":[],"duration":0.01,"type":"transition","attributes":{"direction":0},"name":"none","objectID":"0"}],"automaticPlay":false,"hyperlinks":[],"accessibility":[{"text":"equation.pdf","targetRectangle":{"y":454.45590209960881,"x":768.76740519205589,"width":43.945222409220833,"height":23.231120104904164}},{"text":"Transformer networks describe text sequences of a fully connected graph","targetRectangle":{"y":113.88200378417969,"x":85.833256681755302,"width":1157.0760000000005,"height":43}},{"text":"The embedding at each sequence position, at each layer, is a function of the embeddings at all sequence positions from the previous layer. ","targetRectangle":{"y":156.88200378417969,"x":137.8332566817553,"width":1698.0839999999998,"height":86}},{"text":"The computation that produces the embedding at each sequence position at each layer is referred to as multi-head self attention (more on this in lecture 09).","targetRectangle":{"y":242.88200378417969,"x":137.8332566817553,"width":1767.096,"height":86}},{"text":"This architecture is advantageous from both in terms of computational and optimization.","targetRectangle":{"y":328.88200378417969,"x":173.8332566817553,"width":1378.4039999999995,"height":43}},{"text":"equation.pdf","targetRectangle":{"y":454.45590209960881,"x":526.39802042642975,"width":25.435522409220766,"height":23.231120104904164}},{"text":"pasted-image.tiff","targetRectangle":{"y":518.55828823677018,"x":1432.7841483435725,"width":218.15625,"height":397.3125}},{"text":"equation.pdf","targetRectangle":{"y":994.06436157226494,"x":767.28132120768089,"width":44.01122240922075,"height":18.941120104904144}},{"text":"equation.pdf","targetRectangle":{"y":995.06436157226494,"x":641.62055460611816,"width":44.01122240922075,"height":18.941120104904144}},{"text":"equation.pdf","targetRectangle":{"y":454.45590209960881,"x":391.56629435221168,"width":43.945222409220776,"height":23.231120104904164}},{"text":"equation.pdf","targetRectangle":{"y":995.06436156233087,"x":271.93630240112606,"width":28.969162409220814,"height":18.941120104904144}},{"text":"equation.pdf","targetRectangle":{"y":454.45590209960881,"x":642.10676066080566,"width":43.945222409220833,"height":23.231120104904164}},{"text":"equation.pdf","targetRectangle":{"y":992.45641909095343,"x":516.98916991551437,"width":44.01122240922075,"height":18.941120104904144}},{"text":"￼","targetRectangle":{"y":1038.7800302505493,"x":1890.8129826848599,"width":20.438000000000102,"height":26}},{"text":"Transformer networks","targetRectangle":{"y":4,"x":40,"width":695.31000000000006,"height":84}},{"text":"equation.pdf","targetRectangle":{"y":995.06436156233087,"x":394.68542032440314,"width":44.011222409220807,"height":18.941120104904144}},{"text":"[1] Vaswani et al., 2017","targetRectangle":{"y":960.38910315434111,"x":1024.5837820951151,"width":219.42000000000007,"height":25.799999594688416}},{"text":"Multi-head self attention as presented in the 2017 NIPS paper:“Attention is all you need!” [1]","targetRectangle":{"y":592.77063147227034,"x":1080.5095927615944,"width":259.48800000000006,"height":154.7599995136261}},{"text":"equation.pdf","targetRectangle":{"y":453.62033081054631,"x":266.66449991862049,"width":43.945222409220776,"height":23.231120104904164}}],"baseLayer":{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"objectID":"0","layers":[{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,-0.00035007912466775983,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"layers":[{"animations":[],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"2AF74057B727D5E2D2870400A74FF722"}]}]}}]}