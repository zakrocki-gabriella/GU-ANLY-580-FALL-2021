local_slide( {"name":"78F5F609-A131-4DC7-A78B-EE54B15DD79D","json":{"assets":{"517D965CACE5C4F34DFB9E9324F294FA":{"type":"texture","index":0,"assetRequest":{"type":"slide","state":"contents","slide":"none"},"url":{"native":"assets\/78F5F609-A131-4DC7-A78B-EE54B15DD79D.pdf"},"width":1920,"height":1080},"EB9334459E7BB52797D16F95ABDD7998":{"type":"texture","index":1,"assetRequest":{"type":"slide","state":"contents","slide":"none"},"url":{"native":"assets\/78F5F609-A131-4DC7-A78B-EE54B15DD79D.pdf"},"width":1920,"height":1080}},"events":[{"effects":[{"beginTime":0,"baseLayer":{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"objectID":"0","layers":[{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,-0.00035007912466775983,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"layers":[{"animations":[],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"EB9334459E7BB52797D16F95ABDD7998"},{"animations":[{"additive":false,"timeOffset":0,"beginTime":0,"from":{"scalar":false},"repeatCount":0,"fillMode":"both","duration":0.01,"autoreverses":false,"property":"hidden","to":{"scalar":true},"removedOnCompletion":false}],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"517D965CACE5C4F34DFB9E9324F294FA"}]}]},"effects":[],"duration":0.01,"type":"transition","attributes":{"direction":0},"name":"none","objectID":"0"}],"automaticPlay":false,"hyperlinks":[],"accessibility":[{"text":"equation.pdf","targetRectangle":{"y":981.10484688825591,"x":659.99725668175529,"width":55.0484579439252,"height":43.062130754318105}},{"text":"Neural language modeling: advantages","targetRectangle":{"y":4,"x":40,"width":1242.5700000000004,"height":84}},{"text":"Polysemy","targetRectangle":{"y":120.19548034667969,"x":85.833256681755302,"width":150.73199999999997,"height":43}},{"text":"Word2Vec and other static language models fail to capture polysemy. Having a single embedding per token limits our ability to capture the meaning of words. In fact, our training procedure, whereby identical words in different contexts are constrained to a single representation, introduces ambiguity into the model.","targetRectangle":{"y":163.19548034667969,"x":93.833256681755302,"width":1826.3519999999999,"height":129}},{"text":"Linearity","targetRectangle":{"y":335.19548034667969,"x":85.833256681755302,"width":133.73999999999998,"height":43}},{"text":"Word2Vec, and other similar static word embedding models such as GLOVE, are linear models. In contrast, neural networks are sequences of linear transformations separated by non-linearities; more layers layers translates to more representational power. Without non-linearities, no modeling capacity is gained by adding extra layers.","targetRectangle":{"y":378.19548034667969,"x":93.833256681755302,"width":1825.0919999999996,"height":129}},{"text":"Maximizing the context that a model can consider is an exercise that is largely computational in nature! The DL community has borrowed many of its most successful ideas from the field of discrete-time signal processing, and have built highly efficient numerical methods that utilize modern hardware (GPUs, TPUs).","targetRectangle":{"y":550.19548034667969,"x":49.833256681755302,"width":1862.0639999999999,"height":129}},{"text":"CNNs","targetRectangle":{"y":679.19548034667969,"x":129.8332566817553,"width":92.051999999999992,"height":43}},{"text":"RNNs","targetRectangle":{"y":722.19548034667969,"x":129.8332566817553,"width":89.603999999999985,"height":43}},{"text":"More recently (2017), a mechanism called attention has changed what is possible with language models. Transformer networks, based on this idea, explicitly represent contextual dependencies not only at the input layer but arbitrarily deep into the network. Transformers are the basis behind SOTA methods in NLP and vision! Methods based purely on attention come with the theoretical disadvantage of being able to only consider finite context windows; in practice we can make n large enough (￼) for practical applications, and this context size ends up being larger than what is practical with convolutions and recurrent connections, anyway.","targetRectangle":{"y":808.19548034667969,"x":49.833256681755302,"width":1857.1680000000001,"height":258.97149729589432}},{"text":"￼","targetRectangle":{"y":1038.7800302505493,"x":1893.5739821628085,"width":14.91599999999994,"height":26}}],"baseLayer":{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"objectID":"0","layers":[{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,-0.00035007912466775983,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"layers":[{"animations":[],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":960,"pointY":540},"width":1920,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":1080,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"517D965CACE5C4F34DFB9E9324F294FA"}]}]}}]}} )